{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gym import wrappers, envs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space size is Box(4,)\n",
      "state space size is (24,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalkerHardcore-v3')\n",
    "observation=env.reset()\n",
    "print('action space size is',env.action_space)\n",
    "print('state space size is',env.observation_space.shape)#env.observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for simulation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdinneen/miniconda3/envs/aigym/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('BipedalWalkerHardcore-v3')\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# Turn interactive mode on.\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using', device, 'for simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_threshold (steps, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 200):\n",
    "    '''float -> float\n",
    "    \n",
    "    Return the probabilty of selecting exploration in a RL step.\n",
    "    '''\n",
    "\n",
    "    return min(EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps / EPS_DECAY),1.0)\n",
    "\n",
    "def plot_eps (max_steps=1000,EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 200):\n",
    "    '''int,float,float,int -> None\n",
    "    \n",
    "    Plot the probability of an exploration step vs number of steps.\n",
    "    \n",
    "    max_steps:    Maximum number of steps to plot\n",
    "    EPS_START:    Initial Probabilty at step=0\n",
    "    EPS_END:      Final probability at step=infinity\n",
    "    EPS_DECAY:    Sort of like a Half-life of decay in steps'''\n",
    "    \n",
    "    epsilon=[]\n",
    "    steps=[]\n",
    "    for step in range(max_steps):\n",
    "        steps.append(step)\n",
    "        epsilon.append(epsilon_threshold(step,EPS_START=EPS_START, EPS_END=EPS_END,EPS_DECAY=EPS_DECAY))\n",
    "\n",
    "\n",
    "    fig=plt.figure()\n",
    "    plt.ylim((0,1.05))\n",
    "    plt.xlim((0,max_steps))\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Probability of exploration\")\n",
    "    plt.plot(steps,epsilon)\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "'''A mapping of state-action pairs to next-state reward results'''\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''a cyclic buffer of bounded size that holds recently observed transitions.'''\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            # if we are at less than capacity, allocate fresh space for the transition\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''Randomly return a batch of batch_size from the memory'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''A Deep Q network for predicting actions given states'''\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        # inputs are the 4 vector state space\n",
    "        self.FCL1 = nn.Linear(24,150)\n",
    "        self.FCL2 = nn.Linear(150,120)\n",
    "        self.FCL3 = nn.Linear(120,4)\n",
    "        # Output is a Q value allocated to each action.\n",
    "        #\n",
    "        #       input    hidden   hidden  hidden  output\n",
    "        #       layer    layer1   layer2  layer3  layer\n",
    "        #size    8         16       64      16      4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.FCL1(x))\n",
    "        x = F.relu(self.FCL2(x))\n",
    "        x = self.FCL3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    '''??? -> torch\n",
    "    \n",
    "    Chooses an epsilon-greedy action given an input state.'''\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = min(EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY),1.0)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            action=policy_net(state).max(1)[1].view(1, 1)\n",
    "            return action\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def plot_durations():\n",
    "    '''Show the numder of durations per episode on the yaxis.\n",
    "    After 100 episodes also plot a moving average.\n",
    "    '''\n",
    "    plt.figure(2,figsize=(20,10)) \n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "        \n",
    "def plot_reward(meanscale=10):\n",
    "    '''Show the reward per episode on the yaxis.\n",
    "    After 100 episodes also plot a moving average.\n",
    "    '''\n",
    "    plt.figure(2,figsize=(20,10)) \n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(reward_values)\n",
    "    # Take 100 episode averages and plot them too\n",
    "    #if len(durations_t) >= meanscale:\n",
    "     #   means = sum(total_reward[-meanscale:])/len(meanscale)\n",
    "     #   means = torch.cat((torch.zeros(99), means))\n",
    "     #   plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "\n",
    "def optimize_model():\n",
    "    '''\n",
    "    None -> None\n",
    "    \n",
    "    Update the DQN policy network via SGD.\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        print(\"Warning: We do not have enough history in memory to optimize our network\")\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch \n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = 4\n",
    "# Get length of state space from gym observation space\n",
    "n_states = 24\n",
    "\n",
    "# Create two networks\n",
    "policy_net = DQN(n_states, n_actions).to(device)\n",
    "target_net = DQN(n_states, n_actions).to(device)\n",
    "# Duplicate the weights and biases of the policy net into the target net.\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(),lr=0.001)\n",
    "# Initialise the memory object.\n",
    "memory = ReplayMemory(200000)\n",
    "\n",
    "steps_done = 0\n",
    "episode_durations = []\n",
    "reward_values = []\n",
    "#loss_values =[]\n",
    "action_values=[]\n",
    "frame_values=[]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "# Discount Factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Define epsilon greedy behaviour with 3 parameters.\n",
    "# Note we want to explore exclusively initially.\n",
    "EPS_START = 1.01\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 70.0\n",
    "\n",
    "# Plot what our epsilon values look like.\n",
    "plot_eps(500,EPS_START,EPS_END,EPS_DECAY)\n",
    "\n",
    "# How often do we update our policy network parameters (in steps)\n",
    "TARGET_UPDATE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-8ae57cd24798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mepisode_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mTotalReward\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aigym/lib/python3.8/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/aigym/lib/python3.8/site-packages/gym/envs/box2d/bipedal_walker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_HIP\u001b[0m     \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxMotorTorque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOTORS_TORQUE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorSpeed\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPEED_KNEE\u001b[0m    \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "save_states=5000\n",
    "num_episodes = 1\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    TotalReward=0\n",
    "    episode_frames=[]\n",
    "    #episode_loss=[]\n",
    "    episode_action=[]\n",
    "    #state, reward, done, _ = env.step(1)\n",
    "    state = torch.from_numpy(np.cast['float32'](state)).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        #print(action)\n",
    "\n",
    "        env.render()\n",
    "        if i_episode % save_states == 0:\n",
    "            episode_frames.append(env.render(mode = 'rgb_array'))\n",
    "            episode_action.append(action.item())\n",
    "        print(action)\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        TotalReward+= reward\n",
    "        next_state = torch.from_numpy(np.cast['float32'](next_state)).unsqueeze(0).to(device)\n",
    "        reward = torch.tensor([reward], device=device,dtype=torch.float32)\n",
    "\n",
    "        if not done:\n",
    "            next_state = next_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            reward_values.append(TotalReward)\n",
    "            plot_reward()\n",
    "            break\n",
    "    if i_episode % save_states == 0:\n",
    "        frame_values.append(episode_frames)\n",
    "        #loss_values.append(episode_loss)\n",
    "        action_values.append(episode_action)\n",
    "        \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "plot_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99328005,  0.02572328,  0.21182325, -0.17962882], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
