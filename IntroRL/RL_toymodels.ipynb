{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is my playing around with Reinforcement learning models. It can be thought of as a `hello world` notebook to setup Cuda, CuDNN, aigym etc. I plan to solve a couple of aigym problems using various techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gym import wrappers, envs\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cartpole problem is typically the first problem analysed in RL. It consists of an inverted pendulum on a 1d slide. The aim is to keep the pendulum vertical by moving the cart either left or right in reponse to it falling over. The game ends when the pole's lean exceeds 15 degrees or the slide moves out of the playing area. Below I have implemented a number of techniques for solving this problem in order of increasing complexity.\n",
    "\n",
    "The state space looks like `cart position, cart velocity, pole angle, pole velocity at tip`.\n",
    "\n",
    "The action space looks like `move left, move right`.\n",
    "\n",
    "AiGym keep excellent documentation on each of their environments on their [github page](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dumb Solution with Random actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to run our simulation to completion taking random actions each step. This is to familiarise us with the AIGym, how to start it, how to step it, how to get a reward and how to tell when it is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialise the CartPole-v0 problem.\n",
    "env = gym.make('CartPole-v0')\n",
    "TotalReward=0\n",
    "done=False\n",
    "observation=env.reset()\n",
    "# Loop until the pole falls.\n",
    "while not done:\n",
    "    # Render the environment to screen.\n",
    "    env.render()\n",
    "    # Pause such that this loop creates a 10Hz movie of the system.\n",
    "    time.sleep(0.1)\n",
    "    # Choose a random action\n",
    "    action=env.action_space.sample()\n",
    "    # Enact the chosen action and recieve a reward.\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    TotalReward+=reward\n",
    "    if done:\n",
    "        break\n",
    "    \n",
    "print(\"Reward was: \",TotalReward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Monte Carlo policy search Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next we are going to create a vector with random numbers and take the dot product of this random vector with the present state vector. We are then going to take an action depending on the sign of this dot product. This is a policy search solution.\n",
    "\n",
    "We will then search for the random vector that gives the highest reward.\n",
    "\n",
    "Note that in this example we do not render the environment which leads to a large speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Initialise the CartPole-v0 problem.\n",
    "# Note that this problem ends after 200 steps.\n",
    "env = gym.make('CartPole-v0')\n",
    "bestLength=0\n",
    "episode_lengths=[]\n",
    "\n",
    "best_weights=np.zeros(4)\n",
    "# Loop over 100 random vectors.\n",
    "for i in range(100):\n",
    "    new_weights= np.random.uniform(-1.0,1.0,4)\n",
    "\n",
    "    # try this vector 100 times to give an average reward.\n",
    "    # store the reward of each trial in the list length\n",
    "    length=[]\n",
    "    for j in range(100):\n",
    "        done=False\n",
    "        observation = env.reset()\n",
    "        TotalReward=0\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            # Take dot products of new_weights with observations\n",
    "            # i.e. a 1x4 tensor * 4x1 tensor -> scalar.\n",
    "            # result = +ve move right\n",
    "            #        = -ve move left\n",
    "            action = 1 if np.dot(new_weights,observation) > 0 else 0\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            TotalReward+=reward\n",
    "            if done:\n",
    "                break\n",
    "        length.append(TotalReward)\n",
    "    # Calc. the average reward    \n",
    "    average_length=float(sum(length)/len(length))\n",
    "    # If these weights are better, keep them.\n",
    "    if average_length > bestLength:\n",
    "        bestLength = average_length\n",
    "        best_weights = new_weights\n",
    "    episode_lengths.append(average_length)\n",
    "    if i % 10 == 0 :\n",
    "        print('best length is', bestLength)\n",
    "\n",
    "# Plot the reward history.\n",
    "fig=plt.figure(figsize=(11,7), dpi=100)\n",
    "plt.plot(episode_lengths)\n",
    "plt.xlabel('Random Vector Number')\n",
    "plt.ylabel('Reward')\n",
    "        \n",
    "# Show the best weights in action\n",
    "done=False\n",
    "TotalReward=0\n",
    "observation=env.reset()\n",
    "while not done:\n",
    "    # Show movie @ 50Hz.\n",
    "    env.render()\n",
    "    time.sleep(0.02)\n",
    "    action = 1 if np.dot(best_weights, observation) > 0 else 0\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    TotalReward+=reward\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()        \n",
    "print(\"Game with best weights ran for \",TotalReward, \" turns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is based on the pytorch documentation. I have attempted to remove the Convolutional Neural Net approach to extracting the state space and replaced it with the state space directly from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# Turn interactive mode on.\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using', device, 'for simulation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay memory\n",
    "\n",
    "We’ll be using experience replay memory for training our DQN. It stores the transitions that the agent observes, allowing us to reuse this data later. By sampling from it randomly, the transitions that build up a batch are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "For this, we’re going to need two classses:\n",
    "\n",
    "   `Transition` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the state vector returned from the `env.step()` method.\n",
    "   \n",
    "   `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a .sample() method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A mapping of state-action pairs to next-state reward results'''\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    '''a cyclic buffer of bounded size that holds recently observed transitions.'''\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            # if we are at less than capacity, allocate fresh space for the transition\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''Randomly return a batch of batch_size from the memory'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s define our model. But first, let quickly recap what a DQN is.\n",
    "\n",
    "### DQN algorithm\n",
    "\n",
    "Our environment is deterministic, so all equations presented here are also formulated deterministically for the sake of simplicity. In the reinforcement learning literature, they would also contain expectations over stochastic transitions in the environment.\n",
    "\n",
    "Our aim will be to train a policy that tries to maximize the discounted, cumulative reward \n",
    "\n",
    "$$ R_{t0}=\\sum^{\\infty}_{t=t_0} \\gamma^{t−t_0}r_t $$,\n",
    "\n",
    "where $R_{t0}$ is also known as the return. The discount, $\\gamma$, should be a constant between 0 and 1 that ensures the sum converges. It makes rewards from the uncertain far future less important for our agent than the ones in the near future that it can be fairly confident about.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function \n",
    "\n",
    "$$ Q^∗:State×Action\\rightarrow R$$, \n",
    "\n",
    "that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$$ \\pi^*(s)= \\text{argmax}_a   Q^*(s,a) $$\n",
    "\n",
    "However, we don’t know everything about the world, so we don’t have access to $Q^∗$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^∗$.\n",
    "\n",
    "For our training update rule, we’ll use a fact that every $Q$ function for some policy obeys the Bellman equation:\n",
    "\n",
    "$$ Q^π(s,a)=r+\\gamma Q^π(s′,π(s′)) $$ \n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$: \n",
    "\n",
    "\n",
    "$$ \\delta=Q(s,a)−(r+\\gamma \\text{max}_a Q(s′,a)) $$\n",
    "\n",
    "To minimise this error, we will use the Huber loss. The Huber loss acts like the mean squared error when the error is small, but like the mean absolute error when the error is large - this makes it more robust to outliers when the estimates of Q are very noisy. We calculate this over a batch of transitions, B, sampled from the replay memory:\n",
    "\n",
    "$$ \\mathcal{L}=\\frac{1}{|B|} \\sum_{(s,a,s',r)\\in B} \\mathcal{L}(\\delta) $$\n",
    "\n",
    "\n",
    "$$\\begin{split}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
    "  \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
    "  |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
    "\\end{cases}\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "### Q-network\n",
    "\n",
    "Our model will be a fully connected neural network that takes in state space and has two outputs, representing `Q(s,left)`\n",
    "and `Q(s,right)` (where s is the input to the network). In effect, the network is trying to predict the expected return of taking each action given the current input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''A Deep Q network for predicting actions given states'''\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        # inputs are the 4 vector state space\n",
    "        self.FCL1 = nn.Linear(4,16)\n",
    "        self.FCL2 = nn.Linear(16,64)\n",
    "        self.FCL3 = nn.Linear(64,16)\n",
    "        self.FCL4 = nn.Linear(16,2)\n",
    "        # Output is a Q value allocated to each action.\n",
    "        #\n",
    "        #       input    hidden   hidden  hidden  output\n",
    "        #       layer    layer1   layer2  layer3  layer\n",
    "        #size    4         16       64      16      2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.FCL1(x))\n",
    "        x = F.relu(self.FCL2(x))\n",
    "        x = F.relu(self.FCL3(x))\n",
    "        x = F.relu(self.FCL4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to implement an epsilon-greedy DQN. \n",
    "\n",
    "In this network at each step we can choose to follow the optimal policy given by the policy network(exploit), or we can randomly choose an action in the hope of finding a new state (explore). \n",
    "\n",
    "The below functions show the probability of exploring as a function of episode. Initially we want to exclusively explore as we cannot update out Policy network until we have enough transitions in memory to fit into a batch for SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_threshold (steps, EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 200):\n",
    "    '''float -> float\n",
    "    \n",
    "    Return the probabilty of selecting exploration in a RL step.\n",
    "    '''\n",
    "\n",
    "    return min(EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps / EPS_DECAY),1.0)\n",
    "\n",
    "def plot_eps (max_steps=1000,EPS_START = 0.9, EPS_END = 0.05, EPS_DECAY = 200):\n",
    "    '''int,float,float,int -> None\n",
    "    \n",
    "    Plot the probability of an exploration step vs number of steps.\n",
    "    \n",
    "    max_steps:    Maximum number of steps to plot\n",
    "    EPS_START:    Initial Probabilty at step=0\n",
    "    EPS_END:      Final probability at step=infinity\n",
    "    EPS_DECAY:    Sort of like a Half-life of decay in steps'''\n",
    "    \n",
    "    epsilon=[]\n",
    "    steps=[]\n",
    "    for step in range(max_steps):\n",
    "        steps.append(step)\n",
    "        epsilon.append(epsilon_threshold(step,EPS_START=EPS_START, EPS_END=EPS_END,EPS_DECAY=EPS_DECAY))\n",
    "\n",
    "\n",
    "    fig=plt.figure()\n",
    "    plt.ylim((0,1.05))\n",
    "    plt.xlim((0,max_steps))\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Probability of exploration\")\n",
    "    plt.plot(steps,epsilon)\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "# Discount Factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "# Define epsilon greedy behaviour with 3 parameters.\n",
    "# Note we want to explore exclusively initially.\n",
    "EPS_START = 1.3\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 70.0\n",
    "\n",
    "frame_values=[]\n",
    "\n",
    "# Plot what our epsilon values look like.\n",
    "plot_eps(400,EPS_START,EPS_END,EPS_DECAY)\n",
    "\n",
    "# How often do we update our policy network parameters (in steps)\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get length of state space from gym observation space\n",
    "n_states = env.observation_space.shape[0]\n",
    "\n",
    "# Create two networks\n",
    "policy_net = DQN(n_states, n_actions).to(device)\n",
    "target_net = DQN(n_states, n_actions).to(device)\n",
    "# Duplicate the weights and biases of the policy net into the target net.\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "# Initialise the memory object.\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    '''??? -> torch\n",
    "    \n",
    "    Chooses an epsilon-greedy action given an input state.'''\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = min(EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY),1.0)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        # Exploitation\n",
    "        #print('Exploiting')\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        # Exploration\n",
    "        #print('Exploring')\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    '''Show the numder of durations per episode on the yaxis.\n",
    "    After 100 episodes also plot a moving average.\n",
    "    '''\n",
    "    plt.figure(2,figsize=(20,10)) \n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.01)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    None -> None\n",
    "    \n",
    "    Update the DQN policy network via SGD.\n",
    "    '''\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        print(\"Warning: We do not have enough history in memory to optimize our network\")\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    #                     policy_net(state_batch) \n",
    "    #                        outputs the Q(s_t) for all actions. (i.e. a nx2 tensor)\n",
    "    #                     .gather(1,action_batch) \n",
    "    #                         Selects the Q(s_t) value of the action taken\n",
    "    #                     This gives Q(s_t, a)\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    # THis is a Boolean array with True for non-final states.\n",
    "    # Question: Why does it need to be cast to a tuple?\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # This is a float array containing the state values.\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    # Initialise all values as Zero (i.e. all next_states are final.)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    # Update the non-final states with the predicted values from the current value network.\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    ## Zero all gradients\n",
    "    ### i.e. x.grad =0\n",
    "    optimizer.zero_grad()\n",
    "    ## Perform backward pass (calculate dloss/dx for all parameters x.)\n",
    "    ### i.e x.grad += dloss/dx for all x.\n",
    "    loss.backward()\n",
    "    ## Clip Gradients to +/- 1\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    ## Update all parameters.\n",
    "    ###  i.e. x += -lr * x.grad\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 350\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    \n",
    "    episode_frames=[]\n",
    "\n",
    "    state = env.reset()\n",
    "    \n",
    "    #state, reward, done, _ = env.step(1)\n",
    "    state = torch.from_numpy(np.cast['float32'](state)).unsqueeze(0).to(device)\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        #print(action.item())\n",
    "        env.render()\n",
    "        episode_frames.append(env.render(mode = 'rgb_array'))\n",
    "        # Return the environment parameters.\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "        next_state = torch.from_numpy(np.cast['float32'](next_state)).unsqueeze(0).to(device)\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # If this transition caused the simulation to exit, set the next state to None.\n",
    "        if not done:\n",
    "            next_state = next_state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done or t >= 400:\n",
    "            # Cap simulations at 400 steps.\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            frame_values.append(episode_frames)\n",
    "            break\n",
    "            \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "plot_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(frame_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_durations():\n",
    "    plt.figure(2,figsize=(20,10))\n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.ylim((0,500))\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.1)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "        \n",
    "plot_final_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array2gif import write_gif\n",
    "def save_ep(frames,i):\n",
    "    filename='CartPole_Training_step'+str(i)+'.gif'\n",
    "    write_gif(frames,'output.gif',fps=30)\n",
    "    !convert output.gif -rotate 90 plots/CP/{filename}\n",
    "    #!rm output.gif\n",
    "\n",
    "episodes=[150, 270,271]\n",
    "for i in episodes:\n",
    "    save_ep(frame_values[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del frame_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reward():\n",
    "    '''Show the reward per episode on the yaxis..\n",
    "    '''\n",
    "    plt.figure(2,dpi=200) \n",
    "    #plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Reward values vs training episode: CartPole-v0')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t)\n",
    "    plt.savefig('plots/CP/RewardperEp.png')\n",
    "\n",
    "save_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Deep learning Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is from the pytorch documentation on reinforcement learning.\n",
    "\n",
    "This is a slightly more complicated RL solution: it extracts the state space directly from differences in adjacent rendered images using a convolutional neural net. This is a more general solution to AIGym games and extends to anything with an image output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using', device, 'for simulation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''A Deep Q network to use a CNN to determine Q'''\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input extraction\n",
    "\n",
    "The code below are utilities for extracting and processing rendered images from the environment. It uses the torchvision package, which makes it easy to compose image transforms. Once you run the cell it will display an example patch that it extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can find the main training loop. At the beginning we reset the environment and initialize the state Tensor. Then, we sample an action, execute it, observe the next screen and the reward (always 1), and optimize our model once. When the episode ends (our model fails), we restart the loop.\n",
    "\n",
    "Below, num_episodes is set small. You should download the notebook and run lot more epsiodes, such as 300+ for meaningful duration improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 200\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_durations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
